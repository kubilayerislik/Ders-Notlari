# <mark style="background: #ABF7F7A6;">Çok Katmanlı Algılayıcı</mark>

Bir yapay sinir ağının öğrenmesi istenen olayların girdi ve çıktıları arasındaki ilişler doğrusal olmayan ilişkiler olması durumunda kullanılmaktadır. Günlük hayattaki problemlerin çoğu doğrusal olmadığından dolayı çok katmanlı algılayıcılar en sık kullanılan yapay sinir ağ modelleridir.

Çok katmanlı algılayıcılar günümüzde mühendislik problemlerinin hemen hemen hepsine çözümler üretebilecek bir güce sahiptir. Özellikle sınıflandırma, tanımlama ve genelleme yapmayı gerektiren problemler için çok önemli bir çözüm aracıdır. Çok katmanlı algılayıcılar Delta Öğrenme kuralı adı verilen bir öğrenme yöntemi kullanmaktadır. Temel amacı ağın beklenen çıktısı ile ürettiği çıktı arasındaki hatayı en aza indirmektedir. Bunu hatayı ağa yayarak gerçekleştirdiği için bu ağa hata yayma ağı da denmektedir.

## <mark style="background: #FFF3A3A6;">Çok Katmanlı Algılayıcı Modelinin Yapısı</mark>

Çok Katmanlı Algılayıcı ağlarının yapısı aşağıda gösterildiği gibidir. Çok katmanlı algılayıcı ileriye doğru bağlantılı ve en az 3 katmandan oluşan bir ağdır. 

[![](https://i.imgur.com/pauXRr5m.jpg)](https://i.imgur.com/pauXRr5.png)

__Girdi Katmanı:__ Dış dünyadan gelen girdileri (G1, G2, ..., GN) alarak ara katmana gönderir. Bu katmanda bilgi işleme olmaz. Gelen her bilgi geldiği gibi bir sonraki katmana gönderilir. Birden fazla girdi gelebilir. Her proses elemanının sadece bir tane girdi katmanı ve bir tane çıktı katmanı vardır. Bu çıktı bir sonraki katmanda bulunan bütün proses elemanlarına gönderilir. Girdi katmanındaki her bir proses elemanı bir sonraki katmanda bulunan proses elemanlarının hepsine bağlıdır.

__Ara Katmanlar:__ Ara katmanlar girdi katmanından gelen bilgileri işleyerek bir sonraki katmana gönderir. Bir Çok Katmanlı Algılayıcı ağından birden fazla ara katman ve her katmanda birden fazla proses elemanı olabilir. Ara katmandaki her prorses elemanı bir sonraki katmandaki bütün proses elemanlarına bağlıdır. 

__Çıktı Katmanı:__ Ara katmandan gelen bilgileri işleyerek ağa girdi katmanından verilen girdilere karşılık ağın ürettiği çıktıları (Ç1, Ç2, ÇN) belirleyerek dış dünyaya gönderir. Bir çıktı katmanında birden fazla proses elemanı olabilir. Her proses elemanı bir önceki katmanda bulunan bütün proses elemanlarına bağlıdır. Her proses elemanın sadece bir tane çıktısı vardır.

Çok katmanlı algılıyıcılar denetimli öğrenem stratejisini kullanır. Ağa, hem örnekler hem de örneklerden elde edilmesi gereken çıktılar (beklenen çıktı) verilmektedir. Ağ kendisine gösterilen örneklerden genellemeler yaparak problem uzayını temsil eden bir çözüm uzayı üretmektedir. Daha sonra gösterilen benzer örnekler için bu çözüm uzayı sonuçlar ve çözümler üretebilemektedir.

## <mark style="background: #FFF3A3A6;">Çok Katmanlı Algılayıcı Modelinin Öğrenme Kuralı</mark>

Çok katmanlı algılayıcı ağlar denetimli öğrenme stratejisine göre çalışırlar. Yani; bu ağlara eğitim sırasında hem girdiler hem de o girdilere karşılık üretilmesi gereken (beklenen çıktılar) gösterilir. Ağın görev, her girdi için o girdiye karşılık gelen çıktıyı üretmektir. Çok katmanlı algılayıcı ağının öğrenme kuralı en küçük kareler yöntemine dayalı Delta Öğrenme Kuralının genelleştirilmiş halidir. Ağın öğrenebilmesi için eğitim seti adı verilen ve örneklerden oluşan bir  sete ihtiyaç vardır. Bu set içinde her örnek için ağın hem girdiler hem de o girdiler için üretilmesi gerek  çıktılar belirlenmiştir. Genelleştirilmişt Delta Kuralı iki safhadan oluşmaktadır.

1 safha - ileri doğru hesaplama: Ağın çıktısını hesaplama safhasıdır.
2 safha - geriye doğru hesaplama: Ağırlıklar değiştirme safhasıdır.

### <mark style="background: #D2B3FFA6;">İleri Doğru Hesaplama</mark>

Bu safhada bilgi işleme eğitim setindeki bir örneğin Girdi Katmanından (G1, G2) ağa gösterilmesi ile başlar. Gelen girdiler hiç bir değişiklik olmadan ara katmana gönderilir. Yani girdi katmanındanki _k_. Proses elemanın çıktısı $C_{k}^{i}$ şu şekilde belirlenir.
$$C_{k}^{i} = G_{k}$$
Ara katmandaki her proses elemanı girdi katmanındaki bütün proses elanlarından gelen bilgileri bağlantı ağırlıklarının (A1, A2)  etkisi ile alır. Önce ara katmandaki proses elemanlarına gelen net girdi $\left (NET_{j}^{a} \right )$ şu formül kullanılarak hesaplanır.
$$\left (NET_{j}^{a}=\sum_{k=1}^{n}A_{kj}C_{k}^{i}  \right )$$ Burada $A_{kj}$ _k_ girdi katmanı elemanını _j_ ara katman elemanına bağlayan bağlantının ağırlık değerini göstermektedir. 

### <mark style="background: #D2B3FFA6;">Geriye Doğru Hesaplama</mark>
Ağa sunulan girdi için ağın ürettiği çıktı ağın belenen çıktıları (B1, B2, ....) ile karşılaştırılır. Bunların arasındaki fark hata olarak kabul edilir. Amaç bu hatanın düşürülmesidir. O nedenle geriye hesaplamada bu hata ağın ağırlık değerlerine dağıtılarak bir sonraki iterasyonda hatanın azaltılması sağlanır. Çıktı katmanındaki m proses elemanı için oluşan hata $\left ( E_{m} \right )$
$$E_{m} = B_{m}- C_{m}$$
olacaktır. Bu bir proses elemanı için oluşan hatadır. Çıktı katmanı için oluşan toplam hatayı (TH) bulmak için bütün hataların toplanması gerekir. Bazı hata değerleri negatif olacağından toplamın sıfır olmasını önlemek amacı ile ağırlıkların kareleri hesaplanarak sonucun kare kökü alınır. Çok Katmanlı Algılayıcı ağının eğitilmesideki amaç bu hatayı en azlamaktır. TH şu formül ile bulunur.

$$TH = \frac{1}{2}\sum_{m}^{}E_{m}^{2}$$
Toplam hatayı enazlamak için bu hatanın kendisine neden olan proses elemanlarına dağıtılması gerekmektedir. Bu ise proses elemanlarının ağırlıklarını değiştirmek demektir. Ağın ağırklıklarını değiştirmek için iki durum söz konusudur. 
* Ara katman ile çıktı katmanı arasındaki ağırlıkların değiştirilmesi
* Ara katmanlar arası veya ara katman girdi katmanı arasındaki ağırlıkların değiştirilmesi 

#### <mark style="background: #CACFD9A6;"> Ara Katman ile Çıktı Katmanı Arasındaki Ağırlıkların Değiştirilmesi </mark>

Ara katmandaki j. proses elemanı çıktı katmanındaki m. proses elemanına bağlayan bağlantının ağırlığındaki değişim miktarına  $\Delta A^{a}$ denirse; herhangi bir t zamanında (t. iterasyonunda) ağırlığın değişim miktarı şöyle hesaplanır..

$$\Delta A^{a}_{jm}\left ( t \right )=\lambda \delta_{m}C_{j}^{a}+\alpha \Delta A_{jm}^{a}\left ( t-1 \right )$$

Burada $\lambda$ öğrenme katsayısını, $\alpha$ momentum katsayısını göstermektedir. Öğrenme katsayısı ağırlıkların değişim miktarını, Momentum katsayısı ise Çok Katmanlı Algılayı ağının öğrenmesi esnasında yerel bir optimum noktaya takılığ kalmaması için ağırlık değişim değerlerinin belirli bir oranda bir sonraki değişime eklenmesini sağlarlar. Eşitlikteki $\delta_{m}$ ise m. çıktı ünitesinin hatasını göstermektedir. Şu şekilde hesaplanmaktadır.

$$\delta_{m} = {f}'\left ( NET \right ).E_{m}$$

Buradaki ${f}'\left ( NET \right )$ aktivasyon fonksiyonun türevidir. Sigmoid fonksiyonun kullanılması durumunda;

$$\delta_{m} = C_{m}\left ( 1-C_{m} \right ).E_{m}$$
olacaktır. Değişim miktarı hesaplandıktan sonra ağırlıkların t. iterasyondaki yeni değerleri şöyle olacaktır. 

$$A_{jm}^{a}\left ( t \right ) = A_{jm}^{a}\left ( t-1 \right ) + \Delta A^{a}_{jm}\left ( t \right )$$
Benzer şekilde eşik değer ünitesinin de ağırlıklarını değiştirmek gerekmektedir. Onun için öncelikle değişim miktarını hesaplamak gerekir. Eğer çıktı katmanında bulunan proses elemanlarının eşik değer ağırlıkların $\beta^{c}$ ile gösterilirse; bu initenin çıktısının sabit ve 1 olması nedeni ile değişim miktarı,

$$\Delta \beta^{c}_{m}\left ( t \right )=\lambda \delta_{m}+\alpha \Delta \beta_{m}^{c}\left ( t-1 \right )$$

olacaktır. Eşik değerin t. iterasyondaki ağırlının yeni değeri ise,

$$\beta_{m}^{c}\left ( t \right ) = \beta_{m}^{c}\left ( t-1 \right ) + \Delta \beta^{c}_{m}\left ( t \right )$$

şeklinde hesaplanacaktır.

#### <mark style="background: #CACFD9A6;">Ara Katmanlar Arası veya Ara Katman Girdi Katmanı Arasındaki Ağırlıkların Değiştirilmesi </mark>

Ara katman ile çıktı katmanı arasındaki ağırlıkların değişiminde her ağırlık için sadece çıktı katmanındaki bir proses elemanının hatası dikkate alınmaktadır. Bu hataların oluşumunda girdi katmanı ve ara katman arasındaki ağırlıkların (varsa iki ara katman arasındaki ağırlıkların) payı vardır. Çünkü, en son ara katmana gelen bütün bilgiler girdi katmanı veya önceki ara katmandan gelmektedir.  O nedenle girdi katmanı ile ara katman arasındaki (veya iki ara katman arasındaki) ağırlıkların değiştirilmesinde çıktı katmanındaki proses elemanların hepsinin hatasından payını alması gerekir. Bu ağırlıklardaki değişimi (mesela girdi katmnı ile ara katman arasındaki ağırlıkların değişimi) $\Delta A^{i}$  ile gösterilirse değişim miktarı;

$$\Delta A^{i}_{kj}\left ( t \right )=\lambda \delta_{j}^{a}C_{k}^{i}+\alpha \Delta A_{kj}^{i}\left ( t-1 \right )$$

olacaktır. Burdaki hata terimi $\delta^{a}$ ise şöyle hesaplanacaktır.

$$\delta^{a}_{j} = {f}'\left ( NET \right )\sum_{m} \delta_{m}A^{a}_{jm}$$

Aktivasyyon fonksiyonu olarak sigmoid fonksiyonu düşünülürse bu hata değeri şu şekilde hesaplanacaktır.

$$\delta^{a}_{j} = C^{a}_{j}\left ( 1-C^{a}_{j} \right )  \sum_{m}\delta_{m}A^{a}_{jm}$$

Hata değeri hesaplandıktan sonra yukarıda verilen eşitlik ile değişim miktarını bulmak mümkün olur. Ağırlıkların yeni değerleri ise, 

$$A^{i}_{kj}\left ( t \right )  = A^{i}_{kj}\left ( t-1 \right )  \Delta A^{i}_{kj}\left ( t \right )$$

şeklinde olacaktır. Benzer şekilde, eşik değer ünitesinin yeni ağırlıkları da yukarıdaki gibi hesaplanır. Ara katman eşik değer ağırlıkları $\beta^{a}$ ile gösterilirse değişim miktarı, 

$$\Delta \beta^{a}_{j}\left ( t \right )=\lambda \delta_{j}^{a}+\alpha \Delta \beta_{j}^{a}\left ( t-1 \right )$$

olacaktır Ağırlıkların yeni değerleri ise t. iterasyonda şöyle hesaplanacaktır. 

$$\beta_{j}^{a}\left ( t \right ) = \beta_{j}^{a}\left ( t-1 \right ) + \beta_{j}^{a}\left ( t \right )$$

Böylece ağın ağırlıklarının hepsi değiştirilmiş olacaktır. Bir iterasyon hem ileri hem de geriye hesaplamaları yapılarak tamamlanmış olacaktır. İkinci bir örnek verilerek sonraki iterasyona başlanır ve aynı işlemler öğrenme tamamlanıncaya kadar yinelenir.

## <mark style="background: #FFF3A3A6;">Çok Katmanlı Algılayıcı Ağının Çalışma Prosedürü</mark>

Çok katmanlı ağlarının çalışması şu adımları içermektedir. 
* Örneklerin toplanması: Ağın çözmesi istenilen olay için daha önce gerçekleşmiş örneklerin bulunması adımıdır. Ağın eğitilmesi için örnekler toplandığı gibi (eğitim seti) ağın test edilmesi için de örneklerin için de örneklerin (test seti) toplanması gerekmektedir. Ağın eğitilmesi sırasında test seti ağa hiç gösterilmez. Eğitim setindeki örnekler tek tek gösterilerek ağın olayı öğrenmesi sağlanır. Ağ olayı öğrendikten sonra test setindeki örnekler gösterilerek ağın performansı ölçülür. Hiç görmediği örnekler karşısındaki başarısı ağın iyi öğrenip öğrenmediğini ortaya koymaktadır. 
* Ağın topolojik yapısının belirlenmesi: Öğrenilmesi istenen olay için oluşturulacak olan ağın topolojik yapısı belirlenir. Kaç tane girdi ünitesi, kaç tane ara katman, her ara katmanda kaç tane proses elemanı ve kaç tane çıktı elemanı olması gerektiği bu adımda belirlenmektedir. 
* Öğrenme parametresinin belirlenmesi: Ağın öğrenme katsayısı, proses elemanlarının toplama ve aktivasyon fonksiyonları, momentum katsayısı gibi parametreler bu adımda belirlenmektedir. 
* Ağırlıkların başlangıç değerlerinin atanması: Proses elemanlarını birbirine bağlayan ağırlık değerlerinin ve eşik değer ünitesinin ağırlıklarının başlangıç değerlerinin atanması yapılır. Başlangıçta genellikle rasgele değerler atanır. Daha sonra ağ uygun değerleri öğrenme sırasında kendisi belirler. 
* Öğrenme setinden örneklerin seçilmesi ve ağa gösterilmesi: Ağın öğrenmeye başlaması ve yukarıda anlatılan öğrenme kuralına uygun olarak ağırlıkları değiştirilmesi için ağa örnekler (Girdi / Çıktı değerleri) belirli bir düzeneğe göre gösterilir.
* Öğrenme sırasında ileri hesaplamaların yapılması: Yukarıda anlatıldığı şekilde de sunulan girdi için ağın çıktı değerleri hesaplanır. 
* Gerçekleşen çıktının beklenen çıktı ile karşılaştırılması: Ağın ürettiği hata değerleri bu adımda hesaplanır.
* Ağırlıkların değiştirilmesi: Yukarıda anlatıldığı gibi geri hesaplama yöntemi uygulanarak üretilen atanın azalması için ağırlıkların değiştirilmesi yapılır.

Çok Katmanlı Algılayıcı ağının öğrenmesi tamamlanıncaya, yani gerçekleşen çıktılar ile beklenen çıktılar arasındaki hatalar kabul edilir düzeye ininceye kadar, devam eder. Ağın öğrenmesi için bir durdurma kriterinin olması gerekmektedir. Bu ise genellikle üretilen hatanın belirli bir düzeyin altına düşmesi olarak alınmaktadır.

## <mark style="background: #FFF3A3A6;">Ağın Eğitilmesi</mark>

Ağın kendisine gösterilen girdi örneği için beklenen çıktıyı üretmenisini sağlayacak ağırlık değerleri bulunmaktadır. Başlangıçta bu değerler rasgele atanmakta ve ağa örnekleri gösterdikçe ağın ağırlıkları değiştirilerek zaman içerisinde istenen değerlere ulaşması sağlanmaktadır. İstenen ağırlık değerlerinin ne olduğu bilinmemektedir. Bu nedenle yapay sinir ağlarının davranışlarını yorumlamak ve açıklamak mümkün olmamaktadır. Bunun temel neden bilginin ağın üzerine dağılmış olması ve ağırlık değerlerinin kendi başlarına herhangi bir anlam göstermemeleridir. Ağ ile ilgili bilinen konu problem uzayında en az hata verebilecek ağırlık değerlerinin bulunmasıdır. Hatanın en az değeri kavramını anlatabilmek için basit bir problem düşünülürse, ağın öğrenmesi istenen olayın aşağıdaki şekilde gösterildiği gibi bir hata uzayının olduğu varsayılsın. Şekildeki W* en az hatanın olduğu ağırloık vektörünü göstermektedir.

[![](https://i.imgur.com/KrKxMIsm.jpg)](https://i.imgur.com/KrKxMIs.png)

Ağın W* değerine ulaşması istenmektedir. Bu ağırlık değeri problem için hatanın en az olduğu noktadır. O nedenler her iterasyında $\Delta W$ kadar değişim yaparak hata düzeyinde $\Delta E$ kadar bir hatanın düşmesi sağlanmaktadır. Gerçek hayattaki problemler bu kadar basit ve iki boyutlu olmayacaktır. Aşağıda daha karmaşık bir problemin hata uzayı vermiştir. Görüldüğü gibi problemin çözümü için  en az hatayı veren ağırlık vektörü W* olmasına rağmen pratikte bu hata değerini yakalamak çoğu zaman mümkün değildir. Bu çözüm ağın sahip olabileceği en iyi çözümdür. Fakat bu çözüme nasıl ulaşılacağı konusunda elimizde bir bilgi yoktur. Ağ, eğitim sırasında kendisi bu çözümü yakalamaya çalışmaktadır.

[![](https://i.imgur.com/5JHdhj0m.jpg)](https://i.imgur.com/5JHdhj0.png)

Bazen farklı bir çözüme takılabilmekte ve performansını daha iyileştirmek mümkün olamamaktadır. O nedenle kullanıcılar ağların performanslarında $\varepsilon$ kadar hatayı kabul etmektedir . Tolerans değeri altındaki her hangi bir noktada olay öğrenilmiş kabul edilmektedir. Şekildeki W0 ve W2 çözümlerinin hataları kabul edilebilir hata düzeyinin üzerinde olduğundan bu çözümler kabul edilemez çözümlerdir. Bunlara __yerel çözüm__ denilmektedir. W1 ve W3 çözümleri en iyi çözüm olmamalarına rağmen kabul edilebilir hata düzeyinin altında bir hataya sahiptirler. Bunlarda yerel çözüm olmalarına rağmen __kabul edilebilir çözüm__ olarak adlandırılırlar. Görüldüğü gibi bir problem için birden fazla çözümü üretebilmektedir. Bu nedenle yapay sinir ağlarının her zaman en iyi çözümü ürettikleri söylenemez. Kabul edilebilir bir çözüm ürettiklerini söylemek daha doğrudur. ÜRetilen çözüm en iyi çözüm olsa bile bunu bilmesi zordur. Çoğu durumda bilinmesi mümkün değildir.

__Neden En İyi Çözüm Bulunamamaktadır?__
* Problem eğitilirken bulunna örnekler problem uzayını %100 temsil etmeyebilir.
* Oluşturulan Çok Katmanlı Algılayıcı ağı için doğru parametreler seçilmemiş olabilir.
* Ağın ağırlıkları başlangıçta tam istenildiği şekilde belirlenmemiş olabilir. 
* Ağın topolojisi yetersiz seçilmiş olabilir.

Bazı durumlarda ağın takıldığı yerel sonuç kabul edilebilir hata düzeyinin üstünde kalabilir. Örneğin ağın W0 ağırlıklarının bulunması ve hatanın daha fazla azaltılmasının mümkün olmaması. bu durumda ağın olayı öğrenmesi için bazı değişiklikler yapılarak yenden eğitilmesi gerekir. Bu değişiklikler aşağıdaki gibidir.
* Başka başlangıç değerlerini kullanılabilir.
* Topolojide değişiklikler yapılabilir (ara katman sayısını artırmak, proses elemanı sayısını artırmak veya azaltmak gibi).
* Parametrelerde değişiklik yapılabilir (fonksiyonların başka seçilmesi , öğrenme ve momentum katsayılarının değiştirilmesi).
* Problemin gösterimi ve örneklerin formülasyonu değiştirilerek yeni örnek seti oluşturulabilir.
* Öğrenme setindeki örneklerin sayısı artırılabilir veya azaltılabilir.
* Öğrenme sürecinde örneklerin ağa gösterilmesi

Çok Katmanlı Algılayıcı ağların eğitilmesinde diğer önemli bir sorun ise öğrenme süresinin çok uzun olmasıdır. Ağırlık değerleri başlangıçta büyük değerler olması durumunda ağın yerel sonuçlara düşmesi ve bir yerel sonuçtan diğerine sıçramasına neden olmaktadır. Eğer ağırlıklar küçük aralıkta seçilirse o zaman da ağırlıkların doğru değerleri bulması uzun zaman almaktadır. Bazı problemlerin çözümü sadece 200 iterasyon sürerken bazıları 5 - 10 milyon iterasyon gerektirmektedir. Bu konu da tamamen deneme yanılma yolu ile en uygun başlama koşullarının belirlenmesi gerekmektedir. 

Ağın öğrenmesinin gösterilmesinindeki en güzel yol hata grafiğinin çizizlmesidir. Öğrenen bir ağ için her iterasyonda oluşan hatanın grafiği çizilise aşağıda gösterildiği şekle benzer bir hata grafiği oluşur. Burada hatanın zaman içerisinde düştüğü görülmektedir. Belirli bir iterasyondan sonra hatanın daha fazla azalmadığı görülür. Bu ağın öğrenmesini durdurduğu ve daha iyi bir sonuç bulunamayacağı anlamına gelir. Eğer elde edilen çözüm kabul edilemez ise o zaman ağ yerel bir çözüme takılmış demektir. Bu durumda ağın yukarıda önerilen değişiklikleri yaparak yeniden eğitilmesi gerekmektedir.

[![](https://i.imgur.com/aZr9tZYm.jpg)](https://i.imgur.com/aZr9tZY.png)

## <mark style="background: #FFF3A3A6;">Örnek Problemin Çözümü</mark>
Çok Katmanlı Algılayıcıların bulunmasında nedenlerinden biri olan XOR problemi üzerinde örnek çözüm gerçekleştirilmiştir. Bu problem yapay sinir ağlarında bir devrin kapanıp bir devrin açılmasına neden olmuş önemli bir kilometre taşıdır. Çok Katmanlı Algılayıcı ağının çalışma süreci XOR problemine aşağıdaki gibi uygulanmaktadır.

__1.Adım: Örneklerin Toplanması__

XOR problemi için 4 örnek vardır. Bunlar 1 ve 0 değerlerinden oluşmaktadır. Her örnek için girdiler ve beklenen çıktı aşağıdaki gibidir.

| __Örnekler__ | __Girdi 1__ | __Girdi 2__ | __Çıktı__ |
| ------------ | ----------- | ----------- | --------- |
| Örnek 1      | 0           | 0           | 0         |
| Örnek 2      | 0           | 1           | 1         |
| Örnek 3      | 1           | 0           | 1         |
| Örnek 4      | 1           | 1           | 0         |

__2. Adım: Ağın Topolojik Yapısının Belirlenmesi__
XOR probleminde 2 girdi ve 1 de çıktı olduğundan oluşturulacak olan Çok Katmanlı Algılayıcı ağının da 2 girdi ünitesi ve 1 çıktı ünitesi olacaktır. 1 ara katman ve 2 tanede ara katman proses elemanının bu problemi çözebileceği varsayılmaktadır. Aşağıdaki şekilde ağın topolojisi gösterilmektedir. Görüldüğü gibi ara katman için bir adet, çıktı katmanı için bir adet eşik değer ünitesi vardır.

__3. Adım: Öğrenme Parametrelerinin Belirlenmesi__
Oluşturulan ağ için aktivasyon fonksiyonu olarak sigmoid fonksiyonunun kullanıldığını, öğrenme $\left ( \lambda =0,5 \right )$ ve momentumun $\left ( \alpha = 0,8 \right)$ belirlenmesine karar verilmiştir.

[![](https://i.imgur.com/rIPBFpVm.jpg)](https://i.imgur.com/rIPBFpV.png)
__4. Adım: Ağırlıkların Başlangıç Değerlerinin Atanması__
Oluşturulan ağ için ağrılık vektörleri ve başlangıç değerleri de şu şekilde belirlenmiş olsun. 
Girdi katmanı ile ara katman arasındaki ağırlıklar $A^{i}$ matrisi ile gösterilsin;

$$A^{i}=\begin{bmatrix} 0,129952&0,570345 \\ -0,923123&0,328932 \\ \end{bmatrix}$$

Çıktı katmanı ile ara katman arasındaki ağırlıklar ise $A^{a}$ matrisi ile gösterilsin;

$$A^{a}=\begin{bmatrix} 0,164732&0,752621 \end{bmatrix}$$

Eşik değerin ağırşıkları şöyle olsun.

$$\beta^{a}=\begin{bmatrix} 0,341332&-0,115223 \end{bmatrix} $$
$$\beta^{c}=\begin{bmatrix} 0,993423 \end{bmatrix} $$

__5. Adım: Örneklerin Ağa Gösterilmesi ve İleri Doğru Hesaplama__
Birinci örnek $G1=0$, $G2=0$ ve $B=0$ olarak belirlenmiştir. Bu örnek ağa gösterilirse, ileri doğru hesaplama aşağıda gösterildiği gibi olacaktır.
[![](https://i.imgur.com/AqX5vWvm.jpg)](https://i.imgur.com/AqX5vWv.png)
Ara katman ünitelerinin NET girdileri (eşik değer ünitesinin ağırlık değerleri eklenmiş olarak) şu şekilde hesaplanır.

$$NET_{1} =\left ( 0 \times 0,129952 \right )+\left ( 0 \times -0,923123 \right )+\left ( 1 \times 0,341232 \right ) = 0,342332$$

$$NET_{2} =\left ( 0 \times 0,570345 \right )+\left ( 0 \times -0,328932 \right )+\left ( 1 \times -0,115223 \right ) = -0,115223$$ 

Ara katman ünitelerinin çıktıları ise şöyle hesaplanır.

$$C_{1} = \frac{1}{1+e^{-0,341232}}=0,584490$$
$$C_{2} = \frac{1}{1+e^{0,115223}}=0,471226$$

Çıktı katmanındaki proses elemanının NET girdisi hesaplanırsa;

$$NET =\left ( 1 \times -0,993423 \right )+\left ( 0,584490 \times 0,164732 \right )+\left ( 0,471226 \times 0,752621 \right ) = -0,542484$$

Değeri bulunur. Bu değer ile ağın çıktısı;

$$C = \frac{1}{1+e^{0,542484}}=0,367610$$

Beklenen çıktı 0 olduğuna göre ağın hatası: $E = 0 - 0,367610 = -0,367610$ olur.
Bu hatanın geriye doğru yayılmasın sonucu ara katman ile çıktı katmanı arasındai ağırlıkların değişim miktarı şu şekilde hesaplanır.

$$\delta_{1} = C_{1}\left ( 1 - C_{1} \right ) \times E_{1}$$

$$\delta_{1} = 0,367610 \times \left ( 1 - 0,367610 \right ) \times -0,367610$$

$$\delta_{1} = -0,085459$$

Ağırlık değişim miktarları aşağıdaki formülle hesaplanmaktadır.

$$\Delta A^{i}_{kj}\left ( t \right )=\lambda \delta_{j}^{a}C_{k}^{i}+\alpha \Delta A_{kj}^{i}\left ( t-1 \right )$$
$$\Delta A^{a}_{11}\left (t \right)= 0,5 \times -0,085459 \times 0,584490 + 0,8 \times 0 = -0,024975$$
$$\Delta A^{a}_{21}\left (t \right)= 0,5 \times -0,085459 \times 0,471226+ 0,8 \times 0 = -0,020135$$

Eşik değer ağırlık değişim miktarı aşağıdaki formülle hesaplanmaktadır.

$$\Delta \beta^{a}_{j}\left ( t \right )=\lambda \delta_{j}^{a}+\alpha \Delta \beta_{j}^{a}\left ( t-1 \right )$$
$$\Delta \beta^{c}_{1}\left (t \right)= 0,5 \times -0,085459 + 0,8 \times 0 = -0,042730$$

Ağırlıklardaki bu değişim miktarları ile ara katman ve çıktı katmanı arasındaki ağırlıklar yeniden hesaplanabilir.

$$A^{a}_{11}\left (t \right)= A^{a}_{11}\left (t - 1 \right) + \Delta A^{a}_{11}\left (t \right)$$
$$A^{a}_{11}\left (t \right)= 0,164732 - 0,024975 = 0,139757$$
$$A^{a}_{21}\left (t \right)= 0,752621- 0,020135 = 0,732486$$ 
$$\beta^{c}_{1}\left (t \right)= -0,993423- 0,042730 = - 1,036153$$

Benzer şekilde, girdi katmanı ile ara katman arasındaki ağırlıkların değişim miktarı ve yeni ağırlıklar hesaplanır. Ara katmandaki hata oranları ve değişim miktarları şu şekilde bulunur.

$$\delta_{1}^{a}\left(t \right) = C_{1}\left(1-C_{1}\right)\delta_{1} A_{11}^{a}\left(t-1\right)$$
$$\delta_{1}^{a}= 0,584490 \times \left(1-0,584490\right)\times\left(0,164732\right)\times\left(-0,085459\right) = - 0,034190$$

$$\delta_{2}^{a}= 0,471226\times \left(1-0,471226\right)\times\left(0,752621\right)\times\left(-0,085459\right) = - 0,160263$$
$$\Delta A^{i}_{11}\left(t\right)=0,5\times-0,034190\times0+0,8\times0=0$$
$$\Delta A^{i}_{12}\left(t\right)=0,5\times-0,034190\times0+0,8\times0=0$$
$$\Delta A^{i}_{21}\left(t\right)=0,5\times-0,160236\times0+0,8\times0=0$$
$$\Delta A^{i}_{22}\left(t\right)=0,5\times-0,160236\times0+0,8\times0=0$$
$$\Delta \beta^{i}_{1}\left(t\right)=0,5\times-0,034190\times1=0,017095$$
$$\Delta \beta^{i}_{1}\left(t\right)=0,5\times-0,160236\times1=0,080132$$

Bu değerler kullanılarak ağırlıklar değiştirilir. Ağırlıklardaki değişim miktarı 0 olduğundan ağırlık değerlerinde herhangi bir değişiklik olmayacak ancak eşik değeri ağırlıklarında değişiklik olacaktır. 

$$\beta^{i}_{1}\left(t\right)=0,341232-0,017095 = 0,3242038$$
$$\beta^{i}_{2}\left(t\right)=0,115223-0,081325 = -0,0350905$$

[![](https://i.imgur.com/4Z0aAkUm.jpg)](https://i.imgur.com/4Z0aAkU.png)

Birinci iterasyon bittikten sonra sonra ikinci iterasyon başlayacaktır. Bu kez ikinci örnek ağa gösterilir. $G1=0$, $G2=1$ ve $B=1$ olacaktır. Yukarıdaki işlemler aynı şekilde tekrar edilir. Bu iterasyon bütün çıktılar doğru cevap verinceye kadar devam etmelidir. Aşağıda oluşturulan Çok Katmanlı Algılayıcı ağı öğrendikten sonra ağn ağırlık değerlerini göstermektedir.

[![](https://i.imgur.com/u0enrOQm.jpg)](https://i.imgur.com/u0enrOQ.png)

Bu ağırlıklar ile girdiler ağa tekrar gösterildiğinde o zaman aşağıdaki tabloda gösterilen sonuçlar elde edilir. Bu sonuçlar ağın problemi çok düşük hatalar ile çözebilecek şekilde öğrendiğini göstermektedir.
| Girdi 1 | Girdi 2 | Beklenen Çıktı | Ağın Çıktısı | Hata      |
|---------|---------|----------------|--------------|-----------|
| 0       | 0       | 0              | 0,017622     | -0,017 |
| 1       | 0       | 1              | 0,981504     | 0,018  |
| 0       | 1       | 1              | 0,981491     | 0,018  |
| 1       | 1       | 0              | 0,022782     | -0,022 |

## <mark style="background: #FFF3A3A6;">Ağın Performansının Ölçülmesi</mark>
Bir yapay sinir ağının performansı denilince öğrenme yeteneğinin ölçülmesi anlaşılmaktadır. Ağ eğitim sırasında kendisine gösterilen bütün örneklere doğru cevaplar üretiyor diye performansı iyidir denemez. Bu ağın öğrenip öğrenmediğini gösterebilir ama iyi öğrenip öğrenmediğini göstermez. O nedenle, öğrenen ağların daha önce görmedikleri örnekler karşısında da beklenen performansı gösterip göstermeyeceklerinin ölçülmesi  gerekmektedir. Bunun için geneliikle ağn eğitildiği problem üzerinden hem eğitim için kullanılacak hem de test esnasında kullanılacak örnekler seçilir. eğitim sırasında ağa sadece eğitim setindeki örnekler gösterilir. Ağ bunları öğrenince ( hepsi için veya kabul edilebilir oranda hepsi için doğru cevaplar üretmeye başlayınca ) ağa hiç görmediği test setindeki örnekler gösterilir. Ağın performansı bu görmediği örnekler karşısında ürettiği doğru cevaplar oranı ile şekilde ölçülür.

$$P = \frac{D}{T}\times 100$$

Burada D test setinden doğru olarak cevaplandırılan örnek sayısını, T test setinde bulunan toplam örnek sayısını, P ise performans oranını göstermektedir. Eğer performans oranu (P) istenilen düzeyde ve kabul edilebilir bir değerde değil ise ağ eğitim setindeki bütün örnekleri dğru cevaplasa bile iyi öğrenmiştir denemez. O zaman eğitime birazdaha devam etmek gerekebilir. Eğitim iterasyonlarını artırmaya rağmen hala performans artmıyor ise o zaöam örneklerin problem uzayını iyi temsil edemedikleri veya ağın parametrelerinin veya topolojisinin iyi seçilemediği anlaşılır. Daha önce belirtilen değişikleri yaparak ağı yeniden eğitmek gerekmektedir.


## <mark style="background: #FFF3A3A6;">Çok Katmanlı Algılayıcı Ağının Öğrenmek Yerine Ezberlemesi</mark>

Bazı durumlarda eğitilen Çok Katmanlı Algılayıcı ağı eğitim setindeki bütün örneklere %100 doğru cevap üretmesine rağmen test setindeki örneklere doğru cevaplar üretememektedir. Ancak %10 - %20 civarında bir performans yakalanabilmektedir.  Hatta bu oranlara bile ulaşılamamaktadr. Bu durumda Çok Katmanlı Algılayıcı ağının öğrenmediği fakat öğrenme setini ezberlediği görülmektedir. Çok Katmanlı Algılayıcı ağı tasarımcıları bu durumdan kurtulmak istemektedirler. Çünkü öğrenim %100 görünmekte fakat ağ öğrenmemektedir. Bu ağın günlük kullanıma alınması mümkün değildir. Onun için aşağıda belirtilen konular tekrar gözden geçirilerek ağın ezberlemesinden kurtulmak ve gerçekten öğrenmesini sağlamak gerekir. Çok iyi ezberleyen bir ağ yerine azar azar öğrenen ve kabul edilebilir bir hata ile öğrenme gerçekleştiren performansı düşük ağ daha iyidir.

## <mark style="background: #FFF3A3A6;">Bir Çok Katmanlı Algılayıcı Ağının Oluşturulmasında Dikkat Edilmesi Gereken Bazı Önemli Noktalar</mark>

Bir Çok Katmanlı Algılayıcı Ağının performansını etkileyen unsurlar aşağıdakilerden oluşmaktadır.
* Örneklerin seçilmesi
* Girdi ve çıktıların ağa gösterimi
* Girdilerin nümerik gösterimi
* Çıktıların nümerik gösterimi
* Başlangıç değerlerinin atanması
* Öğrenme ve momentum katsayılarının belirlenmesi
* Örneklerin ağa sunulması
* Ağırlıkların değiştirilme zamanları
* Girdi ve çıktıların ölçeklendirilmesi
* Durdurma Kriterinin belirlenmesi
* Ara katman ve her katmandaki proses elemanlarının sayısının belirlenmesi
* Ağların büyütülmesi veya budanması

#### <mark style="background: #CACFD9A6;">Örneklerin Seçilmesi</mark>

Örnekleirn seçilmesi ağın performansını yakından ilgilendiren bir konudur. Çünkü ağ, bu örnekleri dikkate alarak ağırlıklarını değiştirmektedir. Seçilen örneklerin problem uzayını temsil edebilecek nitelikte olması çok önemlidir. Ağa örneğini göstermediğiniz bir örneğin hakkında yorum yapması beklenmemelidir.  $2\times2=4$ diye ağa öğretirseniz $3\times3$ kaç eder diye soramazsınız. Çünkü ağ bu konuda bir örnek görmemiş ve genellemeleri yapacak durumda olmamıştır. 

Örneğin; Bir Çok Katmanlı Algılayıcı ağına çarpım tablosundan 2 ile çarpım yapması öğretilmek istenmektedir. Bu örnekte problem uzayı toplam 10 örnekten oluşmaktadır. Problem seti eğitim ve test seti olarak aşağıdaki gib ikiye bölünmüştür.

| Eğitim Seti | Test Seti |
|-------------|-----------|
| 2x2=4       | 2x1=2     |
| 2x4=8       | 2x3=6     |
| 2x6=12      | 2x5=10    |
| 2x8=16      | 2x7=14    |
| 2x10=20     | 2x9=18    |

Böyle bir örnek için tasarlanan bir ağ, 200 iterasyonluk öğrenme sonunda kendisine sorulan $2\times3$ kaç eder sorusuna 5,98 gibi bir cevap vermiştir. Bu ise öğrenmenin ne kadar başarılı olduğunu göstemektedir. Çünkü ağ, hiç görmediği bir örnek için %0,2'lik bir hata ile doğru cevapı yakalamıştır. 

Bir yapay sinir ağı tasarlanırken uç değerlerden ve belirli bölgelerdeki örneklerden alınmaktan kaçınılmalıdır. Bu konudaki diğer bir önemli lpmıda seçilen örneklerin ilgili problem uzayında gerçekleşmiş ve çözülmüş gerçek örnekler olmasıdır. Varsayılan çözümler ve örnekler başarılı sonuçların doğmasını önleyebilirler. 

#### <mark style="background: #CACFD9A6;">Girdi ve Çıktı Değerlerinin Sayısallaştırılması</mark>

Örneklerin belirlenmesi kadar belki ondan da daha dönemlisi örneklerin gösteriminin nasıl olacağının belirlenmesidir. Girdi / çıktı çiflerinden oluşan örnekler ağa sadece sayısal olarak gösterilmelidir. Eğer problem uzayında sayısal olmayan faktörlerin dikkate alınması gerekiyorsa o zaaman onların sayısal değerler ile temsil edilebilmesi gerekmektedir.

#### <mark style="background: #CACFD9A6;">Başlangıç Değerlerinin Atanması</mark>
Çok Katmanlı Algılayıcı ağının proses elemanlarını birbirine bağlayan bağlantıların ağırlıklarının başlangıç değerlerinin atanması da ağın performansı ile yakından ilgilidir. Genel olarak ağırlıklar belirli aralıklarda atanmaktadır. Bu aralık eğer büyük tutulursa ağın yerel çözümler arasında sürekli dolaştığı küçük olması durumunda ise öğrenmenin geç gerçekleştiği görülmektedir. Bu değerlerin atanması için henüz belirlenmiş standart bir yöntem yoktur. Ağırlıkların başlangıç değerlerinin rasgele atanmaları istenmektedir. Literatürdeki çalışmalar 1 ile 0,1 arasındaki değerlerin başarılı sonuçlar ürettiğini göstermektedir. Fakat bu tamamen öğrenilmesi istenen problemin niteliğine bağlıdır. 

#### <mark style="background: #CACFD9A6;">Öğrenme Katsayısı ve Momentum Katsayılarının Belirlenmesi</mark>

Başlangıç değerleri kadar öğrenme ve momentum katsayılarının belirlenmesi de ağın öğrenme performansı ile yakından ilgilidir. Öğrenme katsayısı ağırlıkların değişim miktarını belirlemektedir. Eğer büyük seçilirse o zaman yerel çözümler arasında ağın dolaşması ve osilasyon yaşaması söz konusu olmaktadır. Küçük değerler ise öğrenme zamanını artırmaktadır. Literatürde genellikle 0,2 - 0,4 arasındaki değerlerin kullanıldığı görülmektedir. Fakat bu tamamen ilgili probleme bağlıdır. 

Benzer şekilde momentum katsayısı da öğrenmenin performansını etkiler. Momentum katsayısı bir önceki iterasyondaki değişimin belirli bir oranın yeni değişim miktarına eklenmesi olarak görülmektedir. Bu özellikle yerel çözümlere takılan ağların bir sıçrama ile daha iyi sonuçlar bulmasını sağlamak amacı ile önerilmiştir. Bu değerin küçük olması yerel çözümlerden kurtulmayı zorlaştırabilir. Çok büyük değerler ise tek bir çözüme ulaşmada sorunlar yaşatabilir. Literatüre göre 0,6 - 0,8 arasında seçilmesinin uygun olacağını göstermektedir. 

#### <mark style="background: #CACFD9A6;">Örneklerin Ağa Sunulma Şekli</mark>

Örneklerin ağa sunulma şekli de öğrenme performansını etkileyebilir. Genel olarak örnekler ağa iki türlü sunulabilir.
* Sıralı Sunum
* Rasgele Sunum

__Sıralı Sunum__ örnek setindeki birinci örnek ağa sunulur. Bir sonraki iterasyonda ise sıra ile ikinci ve üçüncü sırası ile en sonuncu örneğe örneklerin tamamı ağa sunulur. Sonra tekrar başa dönerek örnek setindeki örnekler tek sıra ile ağa  tekrar sunulur. Bu işlem öğrenme sağlanıncaya kadar devam eder. Bu tür bir sunuçta örnek setindeki bütün örneklerin ağa gösterilme şansları eşittir.

__Rasgele Sunum__ örnekler eğitim seti içinden rasgele seçilirler. Burada da iki durum söz konusudur.
* Seçilen bir örnek tekar set içine atılıp rasgele yeniden seçim yapılır. Bu durumda bir örneğin peş peşe birden fazla defa seçilme şansı vardır. Öğrenme gerçekleşene kadar böyle devam edebilir. Örneklerin ağa gösterilme şansları eşit değildir.
* Rasgele seçilen örnek eğitim içine tekrar atılmaz. Kalanlar arasından rasgele tekrar yeni örnek seçilerek ağa sunulur. Bütün örnekler ağa gösterilince, eğitim seti tekrar içinden rasgele örnekler seçilerek ağa gösterilebilir. Bir gösterilen örnek bütün set ağa gösterilinceye kadar bekler. Öğrenme salanıncaya kadar bu işlem aynı şekilde tekrar eder. Örneklerin ağa gösterilme şansları bu durumda da eşittir.

#### <mark style="background: #CACFD9A6;">Ağırlıkların Değiştirilme Zamanı</mark>

Ağırlıkların değiştirilmesi öğrenme kuralına göre yapılmaktadır. Genel olarak 3 durumda ağırlıkların değiştirilmesine izin verilmektedir. Problemin durumuna göre ağırlıkların ne zaman değiştirileceğine kadar vermek gerekir. Doğru zamanlama ağın performansını etkilemektedir.

* __Her örnek ağa gösterildiğinde (pattern based learning):__  Bu durumda ağa her örnek gösterildiğinde beklenen çıktı ile ağın gerçekleştirdiği çıktı arasındaki hata bulunur ve bu hata ağırlıklarına daha önec anlatılan öğrenme öğrenme kuralı gereğince dağıtılır. İkinci örnek ağa sunulduğunda çıktının hatası hesaplanır ve ağırlıklar değiştirilir. Her örnek gösterimi sonucu ağırlıklar değiştirilir.
* __Belirli sayıda örnek gösterildiğinde (batch based learning):__ Bu durumda ağa her örnek gösterildiğinde hatası hesaplanıp ağırlıklar değiştirilmez. Belirli sayıda örnek tek tek ağa gösterilir ve hatalar toplanırlar. İstenen sayıdaki örneğin ağa gösterilmesi bitince toplana hata ağırlıklara dağıtılır. Genellikle 5 - 10 örnekten oluşan örnek gurupları oluşturulmaktadır. Yani 5 örnek peş peşe ağa gösterilmekte hatalar hesaplanıp toplanmakta ve toplam hata öğrenme kuralına göre ağırlara dağıtılmaktadır. Aynı işlem her örnek grubundaki örneklerin tamamı ağa gösterildikçe tekrarlanmaktadır. 
* __Bütün örnek seti gösterildiğinde (epoch based learning):__ Bu durumda örnek setindeki bütün örnekler ağa tek tek gösterilir. Hatalar hesaplanır ve eğitim setindeki örnekler ağa tek tek gösterilir. Hatalar hesaplanır ve eğitim setindeki örneklerin tamamının hataları toplandıktan sonra bu hata ağırlıklara dağıtılır. Yani; ağın ağırlık değerleri örneklerin tamamı ağa gösterilmektdikçe değiştirilmez. Örnek sayısının az olduğu durumlarda önerilmektedir.

#### <mark style="background: #CACFD9A6;">Örnek Değerlerinin Ölçeklendirilmesi</mark>

Çok katmanlı algılayıcı ağlarında girdi ve çıktıların ölçeklendirilmesi de ağın performansını yakından etkilemektedir. Çünkü ölçeklendirme örnek değerlerinin dağılımını düzenli hale getirmektedir.

#### <mark style="background: #CACFD9A6;">Durdurma Kriterleri</mark>

Çok katmanlı algılayıcı modelinde ağın eğitilmesi kadar gereğinden fazla eğitilmemesi de öenmlidir. Çünkü eğitilmek istenen bir ağ problem uzayına çözüm üretecek ağırlıkları bulduktan sonra eğitime devam edilirse bu ağın ağırlıklarında daha fazla değişikliklere neden olur. Böylece en iyi çözüm üreten bir ağ tekrar daha fermansı düşük ağlara veya öğrenemeyen ağlara dönüşebilir. O denle ağın eğitiminin ne zaman durdurulması gerektiği konusunda da karar vermek gerekmektedir. Bu da ağın başarısını ve performansını etkilemektedir. Pratikte, genel olarak iki türlü durdurma kriteri kullanılmaktadır.

* __Hatanın belirli bir değerin altına düşmesi halinde eğitimin durdurulması:__ Bu durumda hatanın bütün eğitim seti için kabul edilebilir bir değerin altına düşmesi kriter olarak alınmaktadır. Burada bir konuya dikkat etmek gerekir. Bazı durumlarda sadece bir iki örnek için ta değerleri çok küçük seviyelere düşmekte bazı örnekler için ise kabul edilme düzeyde kalmaktadır. Bu durumda eğitime devam etmek gerekir. Eğer eğitim durdurulursa, o zaman diğer örneklerde öğrenme olayı gerçekleşmemiş olur. Pratikte bütün örneklerin hatalarının ortalama değerinin belirli bir değerin altına düşmesi de bu konuda uygulamada görülebilmektedir. En sağlıklısı her öenke için hatayı düşürmektir. Kabul edilebilir hata miktarı problemden probleme değişmekte ve buna tasarımcı karar vermektedir. 
* __Ağın belirli bir iterasyon sayısının tamamlaması sonucu eğitimi durdurma:__ Bu durumda iterasyon sayısının belirlenesi önemli olur. Bir kaç deneme yaptıktan sonra ve hata grafiklerini inceledikten sonra ağın eğitilmesi için gereken eğitim iterasyon sayısı saptanabilir. Özellikle hatanın hangi değerlerin altına düşebileceğinin kestirilmediği durumlarda bu tür bir durdurma kriteri uygulanabilir.

#### <mark style="background: #CACFD9A6;">Ara Katman Sayısının ve Proses Elemanlarının Sayısının Belirlenmesi</mark>

Çok katmanlı algılayıcı modelinde herhangi bir problem için kaç tane ara katman ve her ara katmanda kaç tane proses elemanı kullanılması gerektiğini belirten bir yöntem şu ana kadar bulunmuş değildir. Eğer girdilerin heğsi ikili olursa o zaman bazı yöntemler önerilmekle birlikte bu konudaki çalışmalar deneme yanılma yönteminin etkin olarak kullanıldığını göstermektedir. Ara katman sayısı ve proses elemanı sayıları da ağın performansını yakından ilgilendirmektedir. Bazı durumlarda başlangıçta bir ağ oluşturup zaman içinde büyüterek veya küçültülerek istenen ağa ulaşılır.

#### <mark style="background: #CACFD9A6;">Ağların Büyütülmesi veya Budanması </mark> 

Çok katmanlı algılayıcı ağlarında problemin çözümü için gerekli en iyi topolojiyi belirlemek mümkün olmadığından deneme yanılma yöntemi kullanılma bazen eksik sayıda bazen de fazla sayıda proses elemanı kullanılmaktadır.  Gereken sayıda proses elemanı belirlemek için iki yoldan birisi denenmektedir. Bunlar:

* Küçük bir ağdan başlayıp büyük bir ağa doğru eğitim esnasında sürekli proses eleman sayısını artırmak. 
* Büyük bir ağdan başlayıp küçük bir ağa doğru eğitim sırasında sürekli ağı küçültmek ve proses elemanlarını teker teker ağdan çıkartmak. Buna ağın budanması denmektedir.

#### <mark style="background: #CACFD9A6;">Çok Katmanlı Algılayıcı Ağının Uygulama Alanları </mark> 

Çok katmanlı algılayıcı ağları hayatın hemen hemen her alanında örnekleri görülen bir modeldir. Günümüzde uygulamaların sayısını dahi bilmek mümkün değildir. Genel olarak;
* Sınıflandırma
* Tahmin Etme
* Tanıma
* Yorumlama
* Teşhis Etme

problemlerinde başarılı ile kullanılmaktadır. 
